{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Run a pre-trained PyTorch model in C++\n",
    "\n",
    "In this post we will go through the steps of running a pre-trained PyTorch model in C++ on MacOS (or other platform where you can compile C/C++). The steps are as follows  \n",
    "1. Convert PyTorch model (.pt file) to a TorchScript ScriptModule\n",
    "2. Serialize the the Script Module to a file\n",
    "3. Load the Script Module in C++\n",
    "4. Build/Make the C++ application using CMake\n",
    "\n",
    "This follows the official [PyTorch tutorial](https://pytorch.org/tutorials/advanced/cpp_export.html) but is adapted to our Speech Commands Recognition model. \n",
    "\n",
    "\n",
    "Why would we want to do something like this? There could be several reasons  \n",
    "1. Speed: C/C++ is known to be faster\n",
    "2. Memory footprint: Python is not famous for memory footprint use\n",
    "3. Targeting Edge ML (embedded systems) which don't have a lot of memory or CPU horsepower\n",
    "4. Integrating into a native app (iOS or MacOS)\n",
    "5. Production cloud service\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "# CUSTOMARY IMPORTS\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import os\n",
    "\n",
    "from jumpml import models\n",
    "from jumpml import eval\n",
    "from jumpml import SpeechCommandsDataset as scd\n",
    "from jumpml import utils\n",
    "\n",
    "import itertools\n",
    "from IPython.display import Audio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "random_seed = 1        \n",
    "torch.manual_seed(random_seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Convert PyTorch model (.pt file) to a TorchScript ScriptModule\n",
    "\n",
    "\n",
    "### What is TorchScript?\n",
    "An intermediate representation of a PyTorch model that can be run in C++. We can obtain TorchScript of a PyTorch model (subclass of nn.Module) by  \n",
    "1. Tracing an existing module\n",
    "2. Use scripting to directly compile a module  \n",
    "\n",
    "Tracing is accomplished by creating some sample inputs and then calling the forward method and recording / tracing by a function called torch.jit.trace. The scripting method is useful when there is some control flow (data dependent execution) in the model. We show the tracing method below for our Speech Commands quantized model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOAD MODEL WEIGHTS, QUANTIZE WEIGHTS TO 8-BIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechCommandsModel(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(8, 20), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(32, 8, kernel_size=(4, 10), stride=(1, 1))\n",
       "  (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=1536, out_features=128, bias=True)\n",
       "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=128, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"./models/speech_commands_model.pt\"\n",
    "nnModel = models.SpeechCommandsModel().to(device)       # Instantiate our model and move model to GPU if available\n",
    "nnModel.load_state_dict(torch.load(PATH, map_location=torch.device(device)))\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    nnModel, {nn.Conv2d, nn.Linear}, dtype=torch.qint8)\n",
    "quantized_model.eval()\n",
    "nnModel.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL TRACING WITH INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpeechCommandsModel(\n",
      "  original_name=SpeechCommandsModel\n",
      "  (conv1): Conv2d(original_name=Conv2d)\n",
      "  (bn1): BatchNorm2d(original_name=BatchNorm2d)\n",
      "  (conv2): Conv2d(original_name=Conv2d)\n",
      "  (bn2): BatchNorm2d(original_name=BatchNorm2d)\n",
      "  (fc1): Linear(\n",
      "    original_name=Linear\n",
      "    (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
      "  )\n",
      "  (bn3): BatchNorm1d(original_name=BatchNorm1d)\n",
      "  (fc2): Linear(\n",
      "    original_name=Linear\n",
      "    (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "testFiles = utils.get_filenames('files',searchstr='SCRIC20*')\n",
    "(X,y) = scd.get_file_features(testFiles[0], padLR=False)\n",
    "traced_model = torch.jit.trace(quantized_model, (X))\n",
    "print(traced_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TORCHSCRIPT SCRIPTMODULE INTERMEDIATE REPRESENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    input: Tensor) -> Tensor:\n",
      "  _0 = self.fc2\n",
      "  _1 = self.bn3\n",
      "  _2 = self.fc1\n",
      "  _3 = self.bn2\n",
      "  _4 = self.conv2\n",
      "  _5 = (self.bn1).forward((self.conv1).forward(input, ), )\n",
      "  input0 = torch.max_pool2d(_5, [2], annotate(List[int], []), [0, 0], [1, 1], False)\n",
      "  input1 = torch.relu(input0)\n",
      "  _6 = (_3).forward((_4).forward(input1, ), )\n",
      "  input2 = torch.max_pool2d(_6, [2], annotate(List[int], []), [0, 0], [1, 1], False)\n",
      "  x = torch.relu(input2)\n",
      "  x0 = torch.view(x, [-1, 1536])\n",
      "  x1 = torch.relu((_1).forward((_2).forward(x0, ), ))\n",
      "  _7 = torch.log_softmax((_0).forward(x1, ), 1, None)\n",
      "  return _7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(traced_model.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VERIFY THAT OUTPUTS ARE MATCHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.4839, -4.6787, -1.6807, -6.9479, -0.3579, -4.0282, -8.1275, -5.0048,\n",
      "         -6.2447, -5.5977, -3.1421]])\n",
      "tensor([[-3.4839, -4.6787, -1.6807, -6.9479, -0.3579, -4.0282, -8.1275, -5.0048,\n",
      "         -6.2447, -5.5977, -3.1421]])\n",
      "tensor([[-3.4179, -4.6061, -1.6919, -6.9600, -0.3590, -4.0849, -8.1481, -4.9647,\n",
      "         -6.2618, -5.5800, -3.1247]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(traced_model(X))             # TORCHSCRIPT version of QUANTIZED MODEL\n",
    "print(quantized_model(X))          # QUANTIZED MODEL\n",
    "print(nnModel(X))                  # ORIGINAL MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is special about this TorchScript code? \n",
    "\n",
    "According to the official tutorial, there are several advantages to having a intermediate representation of the model graph\n",
    "\n",
    "1. TorchScript code can be invoked in its own interpreter and many requests can be\n",
    "   processed on the same instance simultaneously due to absence of a global instance lock\n",
    "2. This format allows to save the whole model to disk and load it\n",
    "   into another environment\n",
    "3. TorchScript gives a representation in which we can do compiler\n",
    "   optimizations\n",
    "4. TorchScript allows to interface with many backend/device runtimes\n",
    "\n",
    "Let's take their word for it and keep these in mind for now and remind ourselves later when we see the usecase in action. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialize the the Script Module to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveScriptModule(\n",
      "  original_name=SpeechCommandsModel\n",
      "  (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
      "  (bn1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
      "  (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
      "  (bn2): RecursiveScriptModule(original_name=BatchNorm2d)\n",
      "  (fc1): RecursiveScriptModule(\n",
      "    original_name=Linear\n",
      "    (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
      "  )\n",
      "  (bn3): RecursiveScriptModule(original_name=BatchNorm1d)\n",
      "  (fc2): RecursiveScriptModule(\n",
      "    original_name=Linear\n",
      "    (_packed_params): RecursiveScriptModule(original_name=LinearPackedParams)\n",
      "  )\n",
      ")\n",
      "def forward(self,\n",
      "    input: Tensor) -> Tensor:\n",
      "  _0 = self.fc2\n",
      "  _1 = self.bn3\n",
      "  _2 = self.fc1\n",
      "  _3 = self.bn2\n",
      "  _4 = self.conv2\n",
      "  _5 = (self.bn1).forward((self.conv1).forward(input, ), )\n",
      "  input0 = torch.max_pool2d(_5, [2], annotate(List[int], []), [0, 0], [1, 1], False)\n",
      "  input1 = torch.relu(input0)\n",
      "  _6 = (_3).forward((_4).forward(input1, ), )\n",
      "  input2 = torch.max_pool2d(_6, [2], annotate(List[int], []), [0, 0], [1, 1], False)\n",
      "  x = torch.relu(input2)\n",
      "  x0 = torch.view(x, [-1, 1536])\n",
      "  x1 = torch.relu((_1).forward((_2).forward(x0, ), ))\n",
      "  _7 = torch.log_softmax((_0).forward(x1, ), 1, None)\n",
      "  return _7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "traced_model.save('models/traced_qsc.zip')\n",
    "loaded = torch.jit.load('models/traced_qsc.zip')\n",
    "print(loaded)\n",
    "print(loaded.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Script Module in C++\n",
    "The PyTorch C++ API, also known as LibTorch, is used to load the serialized PyTorch model in C++. The LibTorch distribution consists of shared libraries, headers and build config files. CMake is the recommended build configuration tool. \n",
    "\n",
    "We have a few (boring install) steps to do now  \n",
    "1. Download and install [LibTorch](https://pytorch.org/cppdocs/installing.html). Just a measly 150 MB.\n",
    "2. Install [CMake](https://cmake.org/download/) if you don't have it already\n",
    "\n",
    "Next let us try to compile a simple C++ program just to verify build system:\n",
    "```C++\n",
    "\n",
    "#include <torch/torch.h>\n",
    "#include <iostream>\n",
    "\n",
    "int main() {\n",
    "  torch::Tensor tensor = torch::eye(3);\n",
    "  std::cout << tensor << std::endl;\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
