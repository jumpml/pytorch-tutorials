{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Commands recognition using ConvNets in PyTorch\n",
    "\n",
    "This is a tutorial post on speech commands recognition. There are around 10 speech commands like Yes, No, Up, Down. But first we will need to install torchaudio, which has some convenient dataloaders for the [Speech Commands dataset](https://arxiv.org/abs/1804.03209).\n",
    "\n",
    "The command below should be executed in a terminal to install torchaudio. The -c option searches on the pytorch channel.\n",
    "conda install -c pytorch torchaudio\n",
    "\n",
    "Here's the basic plan\n",
    "1. Data loading and preprocessing: setup the features (log Mel) and labels \n",
    "2. Model specification and loss function: CNN architecture and cross-entropy loss \n",
    "3. Training: minimize the loss function using training data and use validation data to assess training quality\n",
    "4. Evaluate performance on test set\n",
    "5. Real-world evaluation: record from real mic and test \n",
    "\n",
    "This notebook is available on github at this [link](https://github.com/jumpml/pytorch-tutorials/blob/master/SpeechCommands_CNN.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOMARY IMPORTS\n",
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "random_seed = 1        \n",
    "torch.manual_seed(random_seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data  Setup\n",
    "\n",
    "### Step 1: Download the data\n",
    "If we look at the source code for the speechcommands dataset class in torchaudio, we need to specify the version of speech commands (v0.2) and if we want to download the dataset. The dataset is around 2.3GB in size. Each item in the dataloader consists of the following information\n",
    "waveform, sample_rate, label, speaker_id, utterance_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a audio processing pipeline as done in \n",
    "# https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch\n",
    "\n",
    "\n",
    "def process_data(data,sr=8000,nfft=256,hoplen=128,nMel=40):\n",
    "    batchSize = len(data)\n",
    "    audio_transforms = nn.Sequential(\n",
    "                    torchaudio.transforms.Resample(16000, sr),\n",
    "                    torchaudio.transforms.MelSpectrogram(sample_rate=sr,n_fft=nfft,hop_length=hoplen,n_mels=nMel)\n",
    "    )\n",
    "    features=torch.zeros([batchSize, nMel, int(np.ceil(1.*sr/hoplen))])\n",
    "    labels = []\n",
    "    idx = 0\n",
    "    for (waveform,sr,label,sid,utnum) in data:\n",
    "        #print(f'sr={sr} waveform_len={waveform.shape} {sid} {utnum}')\n",
    "        #feature = waveform\n",
    "        #features[idx,:,:] = spec\n",
    "        labels.append(label)\n",
    "        #print(f'At index {idx} we have label= {label}')\n",
    "        idx = idx + 1\n",
    "        \n",
    "    return features, labels\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "speechcommands_data = torchaudio.datasets.SPEECHCOMMANDS('.',\n",
    "                                                         url='speech_commands_v0.02',\n",
    "                                                         folder_in_archive='SpeechCommands',\n",
    "                                                         download=False)\n",
    "data_loader = torch.utils.data.DataLoader(speechcommands_data,\n",
    "                                          batch_size=4000,\n",
    "                                          shuffle=True,\n",
    "                                          collate_fn=lambda x: process_data(x),\n",
    "                                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(data_loader)\n",
    "batch_idx, (features, labels) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'backward', 'left', 'two', 'forward', 'on', 'one', 'visual', 'down', 'four', 'bird', 'wow', 'seven', 'three', 'tree', 'happy', 'six', 'nine', 'house', 'right', 'eight', 'up', 'sheila', 'cat', 'yes', 'learn', 'zero', 'five', 'stop', 'dog', 'go', 'off', 'bed', 'follow', 'no', 'marvin'}\n"
     ]
    }
   ],
   "source": [
    "print(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of spectrogram: torch.Size([40, 63])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-2652d9b5ff5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc485b1c5f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ragh/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/ragh/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/ragh/anaconda3/lib/python3.7/multiprocessing/process.py\", line 140, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/home/ragh/anaconda3/lib/python3.7/multiprocessing/popen_fork.py\", line 48, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/home/ragh/anaconda3/lib/python3.7/multiprocessing/popen_fork.py\", line 28, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of spectrogram: {}\".format(features[0].size()))\n",
    "\n",
    "plt.figure()\n",
    "p = plt.imshow(features[0].log2()[0,:,:].detach().numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105829"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
