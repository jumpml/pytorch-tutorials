{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To convert a pre-trained PyTorch model to CoreML\n",
    "\n",
    "In this post we will go through the steps of converting a pre-trained PyTorch model to Apple's CoreML framework. \n",
    "\n",
    "Why bother? I don't know about you, but I'd rather not worry about whether my model runs efficiently on platform X, Y, or Z. I just want to focus on solving the ML problem and let someone else figure out how to run it efficiently if the hardware has a GPU or NeuralEngine or whatever they come up with next. This is what CoreML allows us to do on the Apple devices, in theory. Once we have a CoreML model, it will run efficiently on a Mac, iPad, iPhone or Watch. Importing and running the converted CoreML model into an App is supposed to be a breeze. Enough talk, let's find out! \n",
    "\n",
    "From [coremltools](https://coremltools.readme.io/docs/pytorch-conversion)\n",
    "> With coremltools 4.0+, you can convert your model trained in PyTorch to the Core ML format directly, without requiring an explicit step to save the PyTorch model in ONNX format. This is the recommended way to convert your PyTorch model to Core ML format\n",
    "\n",
    "We can install coremltools via\n",
    "```terminal\n",
    "pip install --upgrade coremltools\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a simple layer module we'll reuse in our network.\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self, dims):\n",
    "        super(Layer, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(*dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple network consisting of several base layers.\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.layer1 = Layer((3, 6, 3))\n",
    "        self.layer2 = Layer((6, 16, 1))\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.layer1(input)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNet()  # Instantiate the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = torch.rand(1, 3, 224, 224)  # Example input, needed by jit tracer.\n",
    "traced_model = torch.jit.trace(model, example_input)  # Generate TorchScript by tracing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:scikit-learn version 0.23.2 is not supported. Minimum required version: 0.17. Maximum required version: 0.19.2. Disabling scikit-learn conversion API.\n",
      "Converting Frontend ==> MIL Ops:  97%|█████████▋| 31/32 [00:00<00:00, 4511.10 ops/s]\n",
      "Running MIL optimization passes: 100%|██████████| 17/17 [00:00<00:00, 2261.22 passes/s]\n",
      "Translating MIL ==> MLModel Ops: 100%|██████████| 30/30 [00:00<00:00, 23903.71 ops/s]\n",
      "/Users/ragh/anaconda3/lib/python3.6/site-packages/coremltools/models/model.py:119: RuntimeWarning: You will not be able to run predict() on this Core ML model. Underlying exception message was: Error compiling model: \"Error reading protobuf spec. validator error: Layer 'input' produces an output named 'input' which is also an output produced by the layer '__input'.\".\n",
      "  RuntimeWarning,\n"
     ]
    }
   ],
   "source": [
    "import coremltools as ct\n",
    "# Convert using the same API. Note that we need to provide \"inputs\" for pytorch conversion.\n",
    "model_from_torch = ct.convert(traced_model,\n",
    "                              inputs=[ct.TensorType(name=\"input\", shape=example_input.shape)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOMARY IMPORTS\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import os\n",
    "import coremltools as ct\n",
    "\n",
    "from jumpml import models\n",
    "from jumpml import eval\n",
    "from jumpml import SpeechCommandsDataset as scd\n",
    "from jumpml import utils\n",
    "\n",
    "import itertools\n",
    "from IPython.display import Audio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "random_seed = 1        \n",
    "torch.manual_seed(random_seed)\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\" # quantization is not available on GPU\n",
    "print('Using', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Convert PyTorch model (.pt file) to a TorchScript ScriptModule\n",
    "\n",
    "\n",
    "### What is TorchScript?\n",
    "An intermediate representation of a PyTorch model that can be run in C++. We can obtain TorchScript of a PyTorch model (subclass of nn.Module) by  \n",
    "1. Tracing an existing module\n",
    "2. Use scripting to directly compile a module  \n",
    "\n",
    "Tracing is accomplished by creating some sample inputs and then calling the forward method and recording / tracing by a function called torch.jit.trace. The scripting method is useful when there is some control flow (data dependent execution) in the model. We show the tracing method below for our Speech Commands quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = models.SpeechCommandsModel()\n",
    "example_input = torch.rand(1, 1, 64, 101)\n",
    "traced_model = torch.jit.trace(torch_model, example_input)\n",
    "model_from_torch = ct.convert(traced_model,\n",
    "                              inputs=[ct.TensorType(name=\"x\", shape=example_input.shape)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./models/speech_commands_model.pt\"\n",
    "nnModel = models.SpeechCommandsModel().to(device)       # Instantiate our model and move model to GPU if available\n",
    "nnModel.load_state_dict(torch.load(PATH, map_location=torch.device(device)))\n",
    "nnModel.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = torch.rand(1, 1, 64, 101)\n",
    "traced_model = torch.jit.trace(nnModel, example_input)\n",
    "model_from_torch = ct.convert(traced_model,\n",
    "                              inputs=[ct.TensorType(name=\"x\", shape=example_input.shape)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL TRACING WITH INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFiles = utils.get_filenames('files',searchstr='SCRIC20*')\n",
    "(X,y) = scd.get_file_features(testFiles[0], padLR=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction using Core ML\n",
    "out_dict = model_from_torch.predict({\"x\": X})\n",
    "\n",
    "# Print out top-1 prediction\n",
    "print(out_dict[\"out\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the model to Core ML using the Unified Conversion API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools as ct\n",
    "# Convert to Core ML using the Unified Conversion API\n",
    "model = ct.convert(\n",
    "    traced_model,\n",
    "    inputs=[ct.TensorType(name=\"input\", shape=X.shape)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(traced_model(X))             # TORCHSCRIPT version of QUANTIZED MODEL\n",
    "print(quantized_model(X))          # QUANTIZED MODEL\n",
    "print(nnModel(X))                  # ORIGINAL MODEL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
